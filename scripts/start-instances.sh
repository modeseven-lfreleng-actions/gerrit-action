#!/bin/bash
# SPDX-License-Identifier: Apache-2.0
# SPDX-FileCopyrightText: 2025 The Linux Foundation

# Start Gerrit instances script
# This script starts one or more Gerrit server containers based on JSON
# configuration

set -euo pipefail

echo "Starting Gerrit instances..."

# API paths file (populated by detect-api-paths.sh)
API_PATHS_FILE="$WORK_DIR/api_paths.json"

# Custom image name (built from our Dockerfile with uv/gerrit_to_platform)
CUSTOM_IMAGE_NAME="gerrit-extended"
CUSTOM_IMAGE_TAG="${GERRIT_VERSION}"
CUSTOM_IMAGE="${CUSTOM_IMAGE_NAME}:${CUSTOM_IMAGE_TAG}"

# Build custom Gerrit image with uv and gerrit_to_platform
# Check if custom image exists or build it
ensure_custom_image() {
  # First, check if the custom image already exists (built by action.yaml)
  if docker image inspect "${CUSTOM_IMAGE}" >/dev/null 2>&1; then
    echo "Custom image ${CUSTOM_IMAGE} already exists ‚úÖ"
    return 0
  fi

  # Image doesn't exist, try to build it
  # Use BASH_SOURCE[0] instead of $0 for reliable path resolution when sourced
  local dockerfile_dir
  dockerfile_dir="$(dirname "${BASH_SOURCE[0]}")/.."

  # Check if Dockerfile exists
  if [ ! -f "$dockerfile_dir/Dockerfile" ]; then
    echo "::warning::Custom image not found and Dockerfile not available"
    echo "::warning::Falling back to official gerritcodereview/gerrit image"
    CUSTOM_IMAGE="gerritcodereview/gerrit:${GERRIT_VERSION}"
    return 0
  fi

  echo "Building custom Gerrit image with uv and gerrit_to_platform..."
  echo "  Base image: gerritcodereview/gerrit:${GERRIT_VERSION}"
  echo "  Custom image: ${CUSTOM_IMAGE}"

  # Build the custom image
  if docker build \
    --build-arg "GERRIT_VERSION=${GERRIT_VERSION}" \
    -t "${CUSTOM_IMAGE}" \
    -f "$dockerfile_dir/Dockerfile" \
    "$dockerfile_dir" 2>&1; then
    echo "Custom image built successfully ‚úÖ"

    # Verify uv and gerrit-to-platform are available
    # Use --entrypoint="" to prevent Gerrit from starting during verification
    echo "Verifying custom image components..."
    if docker run --rm --entrypoint="" "${CUSTOM_IMAGE}" uv --version 2>/dev/null; then
      echo "  uv: ‚úÖ"
    else
      echo "::warning::uv not found in custom image"
    fi
    if docker run --rm --entrypoint="" "${CUSTOM_IMAGE}" which change-merged 2>/dev/null; then
      echo "  gerrit-to-platform: ‚úÖ"
    else
      echo "::warning::gerrit-to-platform not found in custom image"
    fi
  else
    echo "::warning::Failed to build custom image, falling back to official image"
    CUSTOM_IMAGE="gerritcodereview/gerrit:${GERRIT_VERSION}"
  fi
}

# Ensure custom image is available before starting instances
ensure_custom_image

# Function to get API path for a slug
get_api_path() {
  local slug="$1"
  if [ -f "$API_PATHS_FILE" ]; then
    jq -r ".\"$slug\".api_path // \"\"" "$API_PATHS_FILE"
  else
    echo ""
  fi
}

# Function to get API URL for a slug
get_api_url() {
  local slug="$1"
  if [ -f "$API_PATHS_FILE" ]; then
    jq -r ".\"$slug\".api_url // \"\"" "$API_PATHS_FILE"
  else
    echo ""
  fi
}

# Parse configuration
INSTANCES=$(echo "$GERRIT_SETUP" | jq -c '.[]')
INSTANCE_INDEX=0

# Initialize tracking files
CONTAINER_IDS_FILE="$WORK_DIR/container_ids.txt"
INSTANCES_JSON_FILE="$WORK_DIR/instances.json"
: > "$CONTAINER_IDS_FILE"
echo "{}" > "$INSTANCES_JSON_FILE"

# Setup authentication
setup_ssh_auth() {
  local instance_dir="$1"
  local gerrit_host="$2"

  # Create SSH directory
  mkdir -p "$instance_dir/ssh"
  chmod 700 "$instance_dir/ssh"

  # Write private key
  echo "$SSH_PRIVATE_KEY" > "$instance_dir/ssh/id_rsa"
  chmod 600 "$instance_dir/ssh/id_rsa"

  # Setup known_hosts
  if [ -n "${SSH_KNOWN_HOSTS:-}" ]; then
    echo "$SSH_KNOWN_HOSTS" > "$instance_dir/ssh/known_hosts"
  else
    # Auto-fetch host key
    echo "Auto-fetching SSH host key for $gerrit_host..."
    ssh-keyscan -H "$gerrit_host" 2>/dev/null \
      > "$instance_dir/ssh/known_hosts" || {
      echo "Warning: Could not fetch SSH host key for $gerrit_host"
      touch "$instance_dir/ssh/known_hosts"
    }
  fi
  chmod 644 "$instance_dir/ssh/known_hosts"

  # Create SSH config
  cat > "$instance_dir/ssh/config" <<EOF
Host $gerrit_host
  HostName $gerrit_host
  User git
  IdentityFile /var/gerrit/ssh/id_rsa
  StrictHostKeyChecking yes
  UserKnownHostsFile /var/gerrit/ssh/known_hosts
EOF
  chmod 600 "$instance_dir/ssh/config"
}

# Fetch project list from remote Gerrit server
# This is needed for replicateOnStartup because FetchAll iterates over
# projectCache.all() - projects must exist locally before they can be replicated
fetch_remote_projects() {
  local gerrit_host="$1"
  local api_path="${2:-}"
  local project_filter="${3:-}"
  local max_projects="${4:-100}"  # Limit for safety

  echo "Fetching project list from $gerrit_host..." >&2

  # Build API URL
  local api_url
  if [ -n "$api_path" ]; then
    api_path="${api_path#/}"  # Remove leading slash
    api_url="https://${gerrit_host}/${api_path}/projects/"
  else
    api_url="https://${gerrit_host}/projects/"
  fi

  # Add query parameters
  local query_params="n=${max_projects}"
  if [ -n "$project_filter" ] && [ "$project_filter" != ".*" ]; then
    # Always use regex param (r=) when a filter is provided
    # The caller is responsible for stripping the "regex:" prefix if present
    # and only passing filters that should be treated as regex patterns
    query_params="${query_params}&r=$(printf '%s' "$project_filter" | jq -sRr @uri)"
  fi

  local full_url="${api_url}?${query_params}"
  echo "  API URL: $full_url" >&2

  # Fetch projects (Gerrit API returns )]}' prefix for XSSI protection)
  # Build curl args with authentication based on auth_type
  local -a curl_args
  curl_args=(-s --connect-timeout 30 --max-time 60)

  case "${AUTH_TYPE:-}" in
    http_basic)
      if [ -n "${HTTP_USERNAME:-}" ] && [ -n "${HTTP_PASSWORD:-}" ]; then
        curl_args+=(-u "${HTTP_USERNAME}:${HTTP_PASSWORD}")
        echo "  Using HTTP basic authentication" >&2
      fi
      ;;
    bearer_token)
      if [ -n "${BEARER_TOKEN:-}" ]; then
        curl_args+=(-H "Authorization: Bearer ${BEARER_TOKEN}")
        echo "  Using bearer token authentication" >&2
      fi
      ;;
    ssh)
      # SSH auth doesn't apply to REST API calls
      # Most Gerrit servers allow anonymous project listing
      echo "  Using anonymous access for REST API" >&2
      ;;
  esac

  local response
  response=$(curl "${curl_args[@]}" "$full_url" 2>/dev/null) || {
    echo "  Warning: Failed to fetch project list from $gerrit_host" >&2
    return 1
  }

  # Remove XSSI protection prefix and parse JSON
  local projects
  projects=$(echo "$response" | tail -n +2 | jq -r 'keys[]' 2>/dev/null) || {
    echo "  Warning: Failed to parse project list response" >&2
    return 1
  }

  local count
  count=$(echo "$projects" | grep -c . || echo "0")
  echo "  Found $count projects on remote server" >&2

  # Return project list (one per line)
  echo "$projects"
}

# Generate replication.config (used by both replication and pull-replication plugins)
generate_replication_config() {
  local config_file="$1"
  local slug="$2"
  local gerrit_host="$3"
  local project="${4:-}"
  local remote_ssh_user="${5:-gerrit}"
  local remote_ssh_port="${6:-29418}"

  # Get API path for this instance (detected by detect-api-paths.sh)
  # Note: API path is only used for HTTP URLs, NOT for SSH URLs
  local api_path
  api_path=$(get_api_path "$slug")

  # Determine authentication type (default to ssh if not set)
  local auth_type="${AUTH_TYPE:-ssh}"
  auth_type="${auth_type,,}"

  # Build the replication URL based on auth type
  # For SSH auth, use Gerrit's SSH endpoint (no api_path - SSH doesn't use HTTP context paths)
  # For HTTP-based auth (basic, bearer, etc.), use authenticated git-over-https with /a/
  local git_url
  if [ "$auth_type" = "ssh" ]; then
    # SSH URL format: ssh://<user>@<host>:<port>/${name}.git
    # Note: SSH URLs do NOT include api_path - that's an HTTP-only concept
    git_url="ssh://${remote_ssh_user}@${gerrit_host}:${remote_ssh_port}/\${name}.git"
  else
    # For HTTP Basic/Bearer auth, Gerrit uses /a/ prefix for authenticated access
    # URL format: https://<host>/<api_path>/a/${name}.git
    if [ -n "$api_path" ]; then
      # Normalize api_path: strip leading and trailing slashes
      api_path="${api_path#/}"
      api_path="${api_path%/}"
      git_url="https://${gerrit_host}/${api_path}/a/\${name}.git"
    else
      git_url="https://${gerrit_host}/a/\${name}.git"
    fi
  fi

  # Parse sync_refs from environment
  IFS=',' read -ra REFS <<< "$SYNC_REFS"

  # Get fetch interval (default to 60s if not set)
  # 0 or 0 with any unit (0s, 0m, 0h) disables automatic polling
  local fetch_interval="${FETCH_EVERY:-60s}"
  local fetch_every_enabled=true

  # Validate format first
  if ! [[ "$fetch_interval" =~ ^[0-9]+[smhSMH]?$ ]]; then
    echo "::error::Invalid fetch_every value: '$fetch_interval'. Expected format: <integer>[s|m|h], e.g. 60s, 5m, 1h, or 0 to disable" >&2
    return 1
  fi

  # Check if fetchEvery should be disabled (0 with any unit means disabled)
  # Match: 0, 0s, 0S, 0m, 0M, 0h, 0H
  if [[ "$fetch_interval" =~ ^0[smhSMH]?$ ]]; then
    fetch_every_enabled=false
    echo "  fetchEvery disabled (interval set to $fetch_interval)" >&2
  fi

  cat > "$config_file" <<EOF
# Pull-replication configuration
# Auto-generated by gerrit-server-action
#
# This configuration uses fetchEvery for polling-based replication.
# The plugin will poll the source Gerrit at the configured interval
# to fetch any new or changed refs. This approach:
# - Enables full web UI access (non-replica mode)
# - Provides automatic, self-healing replication
# - Does not require apiUrl (which is mutually exclusive with fetchEvery)

[gerrit]
  replicateOnStartup = $SYNC_ON_STARTUP
  autoReload = true

[replication]
  lockErrorMaxRetries = 5
  maxRetries = 5
  useCGitClient = false
  refsBatchSize = 50

[remote "$slug"]
  url = ${git_url}
EOF

  # Add fetchEvery if enabled (not 0/0s)
  # Must be added before other remote settings
  if [ "$fetch_every_enabled" = "true" ]; then
    echo "  fetchEvery = $fetch_interval" >> "$config_file"
    echo "  Fetch interval (polling): $fetch_interval" >&2
  else
    echo "  Automatic polling disabled" >&2
  fi

  # Continue with remaining remote settings
  # Calculate connectionTimeout to be at least as large as timeout
  # timeout is in seconds, connectionTimeout is in milliseconds
  local timeout_ms=$((REPLICATION_TIMEOUT * 1000))
  local connection_timeout_ms=$timeout_ms
  if [ "$connection_timeout_ms" -lt 120000 ]; then
    connection_timeout_ms=120000  # Minimum 2 minutes
  fi

  cat >> "$config_file" <<EOF
  timeout = $REPLICATION_TIMEOUT
  connectionTimeout = $connection_timeout_ms
  replicationDelay = 0
  replicationRetry = 60
  threads = $REPLICATION_THREADS
  createMissingRepositories = true
  replicateHiddenProjects = false
EOF

  echo "  Git URL for replication: $git_url" >&2

  # Note: apiUrl is NOT used because it's mutually exclusive with fetchEvery
  # The fetchEvery setting enables polling-based replication which:
  # 1. Works without replica mode (web UI remains functional)
  # 2. Automatically polls for changes at the configured interval
  # 3. Is ideal for mirroring from production Gerrit servers

  # Add fetch refspecs
  for ref in "${REFS[@]}"; do
    echo "  fetch = $ref" >> "$config_file"
  done

  # Add project filter if specified
  if [ -n "$project" ]; then
    echo "  projects = $project" >> "$config_file"
  fi
}

# Generate secure.config for authentication
generate_secure_config() {
  local config_file="$1"
  local slug="$2"

  case "$AUTH_TYPE" in
    http_basic)
      cat > "$config_file" <<EOF
[remote "$slug"]
  username = $HTTP_USERNAME
  password = $HTTP_PASSWORD
EOF
      ;;
    bearer_token)
      cat > "$config_file" <<EOF
[auth]
  bearerToken = $BEARER_TOKEN
EOF
      ;;
    ssh)
      # No secure.config needed for SSH
      touch "$config_file"
      ;;
  esac

  chmod 600 "$config_file"
}

# Download pull-replication plugin
download_plugin() {
  local plugin_dir="$1"

  if [ "$SKIP_PLUGIN_INSTALL" = "true" ]; then
    echo "Skipping plugin download (skip_plugin_install=true)"
    return 0
  fi

  mkdir -p /tmp/gerrit-plugins
  local plugin_jar="/tmp/gerrit-plugins/pull-replication-${PLUGIN_VERSION}.jar"

  if [ -f "$plugin_jar" ]; then
    echo "Using cached plugin: $plugin_jar"
    cp "$plugin_jar" "$plugin_dir/pull-replication.jar"
    return 0
  fi

  echo "Downloading pull-replication plugin..."
  local plugin_url="https://gerrit-ci.gerritforge.com/job/plugin-pull-replication-gh-bazel-${PLUGIN_VERSION}/lastSuccessfulBuild/artifact/bazel-bin/plugins/pull-replication/pull-replication.jar"

  if curl -f -L -o "$plugin_jar" "$plugin_url" 2>/dev/null; then
    echo "Plugin downloaded ‚úÖ"
    cp "$plugin_jar" "$plugin_dir/pull-replication.jar"
  else
    echo "::warning::Failed to download plugin from CI"
    echo "::warning::attempting alternate source..."
    # Try alternate source (GitHub releases)
    local alt_url="https://github.com/GerritForge/pull-replication/releases/download/${PLUGIN_VERSION}/pull-replication.jar"
    if curl -f -L -o "$plugin_jar" "$alt_url" 2>/dev/null; then
      echo "Plugin downloaded from alternate source ‚úÖ"
      cp "$plugin_jar" "$plugin_dir/pull-replication.jar"
    else
      echo "::error::Failed to download pull-replication plugin ‚ùå"
      return 1
    fi
  fi
}

# Download more plugins
download_additional_plugins() {
  local plugin_dir="$1"

  if [ -z "$ADDITIONAL_PLUGINS" ]; then
    return 0
  fi

  echo "Downloading additional plugins..."
  IFS=',' read -ra PLUGINS <<< "$ADDITIONAL_PLUGINS"

  for plugin_url in "${PLUGINS[@]}"; do
    plugin_name=$(basename "$plugin_url")
    echo "Downloading: $plugin_name"

    if curl -f -L -o "$plugin_dir/$plugin_name" "$plugin_url" 2>/dev/null; then
      echo "  ‚úÖ $plugin_name"
    else
      echo "  ::warning::Failed to download $plugin_name"
    fi
  done
}

# Initialize Gerrit site
init_gerrit_site() {
  local instance_dir="$1"
  local slug="$2"
  local http_port="$3"
  local canonical_url="$4"

  echo "Initializing Gerrit site for $slug..."

  # Create directory structure with proper Gerrit ownership
  # The official gerritcodereview/gerrit image uses UID:GID 1000:1000
  local GERRIT_UID=1000
  local GERRIT_GID=1000

  mkdir -p "$instance_dir"/{git,cache,index,data,etc,logs,plugins,tmp}
  chown -R "$GERRIT_UID:$GERRIT_GID" "$instance_dir"
  chmod -R 755 "$instance_dir"

  # Run Gerrit init using the container's entrypoint
  # Mount only specific subdirectories to avoid hiding /var/gerrit/bin
  docker run --rm \
    -v "$instance_dir/git:/var/gerrit/git" \
    -v "$instance_dir/cache:/var/gerrit/cache" \
    -v "$instance_dir/index:/var/gerrit/index" \
    -v "$instance_dir/data:/var/gerrit/data" \
    -v "$instance_dir/etc:/var/gerrit/etc" \
    -v "$instance_dir/logs:/var/gerrit/logs" \
    -v "$instance_dir/plugins:/var/gerrit/plugins" \
    -e CANONICAL_WEB_URL="$canonical_url" \
    "${CUSTOM_IMAGE}" \
    init || {
    echo "::error::Failed to initialize Gerrit site for $slug ‚ùå"
    return 1
  }

  echo "Gerrit site initialized ‚úÖ"
}

# Configure Gerrit
configure_gerrit() {
  local instance_dir="$1"
  local slug="$2"
  local http_port="$3"
  local canonical_url="$4"
  local listen_url="$5"
  local api_path="$6"
  local advertised_ssh_addr="$7"  # Format: host:port (e.g., localhost:29418 or tunnel:12345)

  echo "Configuring Gerrit for $slug..."

  local config_file="$instance_dir/etc/gerrit.config"

  if [ -n "$api_path" ]; then
    echo "  URL prefix: $api_path (mirroring production server)"
  else
    echo "  URL prefix: (none)"
  fi

  # Update gerrit.config
  git config -f "$config_file" gerrit.instanceId "$slug"
  git config -f "$config_file" gerrit.canonicalWebUrl "$canonical_url"
  git config -f "$config_file" httpd.listenUrl "$listen_url"
  git config -f "$config_file" sshd.listenAddress "*:29418"

  # Configure advertised SSH address so clone URLs show correctly
  # This tells clients what address/port to use for SSH clones
  # When using tunnels, this will be the tunnel host:port
  git config -f "$config_file" sshd.advertisedAddress "$advertised_ssh_addr"

  # Enable both HTTP and SSH download schemes in the web UI
  # By default Gerrit shows SSH, HTTP, and Anonymous HTTP
  git config -f "$config_file" download.scheme "ssh"
  git config -f "$config_file" --add download.scheme "http"

  # Configure download commands shown in the UI
  git config -f "$config_file" download.command "checkout"
  git config -f "$config_file" --add download.command "cherry_pick"
  git config -f "$config_file" --add download.command "pull"

  # Set auth to development mode for testing
  git config -f "$config_file" auth.type "DEVELOPMENT_BECOME_ANY_ACCOUNT"

  # Container user - run as the gerrit user (UID 1000) for security
  # Directories are chowned to match this UID during init_gerrit_site
  git config -f "$config_file" container.user "gerrit"

  # Enable replication plugin
  git config -f "$config_file" plugin.pull-replication.enabled "true"

  # NOTE: We intentionally do NOT set container.replica=true
  #
  # While replica mode would enable automatic replicateOnStartup (FetchAll),
  # it also disables the web UI entirely (Gerrit becomes read-only).
  #
  # Instead, we use fetchEvery polling which provides:
  # - Automatic replication at configured intervals
  # - Full web UI access (non-replica mode)
  # - Self-healing sync (retries on next poll cycle)
  #
  # The tradeoff is that initial sync takes up to one poll interval,
  # but this is acceptable for CI/CD testing scenarios.
  #
  # Previous (disabled):
  # git config -f "$config_file" container.replica "true"

  echo "Gerrit configured ‚úÖ"
  echo "  Mode: non-replica (web UI enabled)"
  echo "  Replication: fetchEvery polling"
}

# Fetch project list from remote Gerrit and pre-create directories for replication
# This is REQUIRED because fetchEvery mode only polls repos that exist in projectCache.
# Without pre-creating these directories, the plugin won't know about them and won't fetch.
fetch_expected_project_count() {
  local instance_dir="$1"
  local project="$2"
  local gerrit_host="$3"
  local slug="$4"

  echo "Fetching expected project count from remote..."

  local projects_found=()

  # Configurable limit for project fetching (default: 500)
  # Set MAX_PROJECTS environment variable to override
  local max_projects="${MAX_PROJECTS:-500}"

  if [ -z "$project" ]; then
    # No specific project filter - fetch ALL projects from remote server
    echo "  No project filter, fetching full project list..."
    echo "  (Max projects: $max_projects - set MAX_PROJECTS env to override)"

    # Get API path for this instance
    local api_path
    api_path=$(get_api_path "$slug")

    # Fetch remote projects
    local remote_projects
    if remote_projects=$(fetch_remote_projects "$gerrit_host" "$api_path" "" "$max_projects"); then
      # Convert newline-separated list to array
      while IFS= read -r proj; do
        [ -n "$proj" ] && projects_found+=("$proj")
      done <<< "$remote_projects"
    else
      echo "  Warning: Could not fetch remote project list"
      echo "0" > "$instance_dir/expected_project_count"
      return 0
    fi
  else
    # Handle comma-separated project list or single project/pattern
    # Check if it's explicitly marked as a regex pattern with "regex:" prefix
    # This avoids misclassifying literal project names like "my.project" or "name[v2]"
    if [[ "$project" == regex:* ]]; then
      local regex_pattern="${project#regex:}"
      echo "  Project filter explicitly marked as regex: $regex_pattern"
      echo "  Fetching matching projects from remote..."

      local api_path
      api_path=$(get_api_path "$slug")

      local remote_projects
      if remote_projects=$(fetch_remote_projects "$gerrit_host" "$api_path" "$regex_pattern" "$max_projects"); then
        while IFS= read -r proj; do
          [ -n "$proj" ] && projects_found+=("$proj")
        done <<< "$remote_projects"
      else
        echo "  Warning: Could not fetch filtered project list"
        echo "0" > "$instance_dir/expected_project_count"
        return 0
      fi
    else
      # Literal project name(s) - could be comma-separated
      IFS=',' read -ra PROJECTS <<< "$project"
      for proj in "${PROJECTS[@]}"; do
        proj=$(echo "$proj" | xargs)  # trim whitespace
        [ -n "$proj" ] && projects_found+=("$proj")
      done
    fi
  fi

  # Filter out Gerrit internal projects that are not counted during verification
  # (All-Projects and All-Users are local system repos, not replicated from remote)
  local filtered_projects=()
  for proj in "${projects_found[@]}"; do
    if [[ "$proj" == "All-Projects" || "$proj" == "All-Users" ]]; then
      continue
    fi
    filtered_projects+=("$proj")
  done

  # Store expected project count for later verification
  local expected_count=${#filtered_projects[@]}
  echo "  Found $expected_count projects on remote server (excluding All-Projects/All-Users)"
  echo "$expected_count" > "$instance_dir/expected_project_count"

  # Pre-create empty bare repositories for each project
  # This is REQUIRED because fetchEvery mode only polls repos that exist in projectCache.
  # Without pre-creating these directories, the plugin won't know about them and won't fetch.
  # The createMissingRepositories setting only helps when a fetch is attempted, but
  # fetchEvery won't attempt to fetch repos it doesn't know about.
  echo "  Pre-creating project directories for replication..."
  local created_count=0
  for proj in "${filtered_projects[@]}"; do
    local project_dir="$instance_dir/git/${proj}.git"
    if [ ! -d "$project_dir" ]; then
      mkdir -p "$project_dir"
      git init --bare "$project_dir" >/dev/null 2>&1
      # Set proper ownership for Gerrit user (UID:GID 1000:1000)
      chown -R 1000:1000 "$project_dir"
      chmod -R 755 "$project_dir"
      created_count=$((created_count + 1))
    fi
  done
  echo "  Created $created_count project directories"
}

# Start single instance
start_instance() {
  local instance_json="$1"
  local index="$2"

  # Parse instance config
  local project
  project=$(echo "$instance_json" | jq -r '.project // ""')
  local slug
  slug=$(echo "$instance_json" | jq -r '.slug')
  local gerrit_host
  gerrit_host=$(echo "$instance_json" | jq -r '.gerrit')

  # Get remote SSH settings (per-instance override or global default)
  # These are for connecting TO the remote Gerrit server for pull-replication
  local remote_ssh_user
  remote_ssh_user=$(echo "$instance_json" | jq -r '.ssh_user // ""')
  if [ -z "$remote_ssh_user" ]; then
    remote_ssh_user="${REMOTE_SSH_USER:-gerrit}"
  fi
  local remote_ssh_port
  remote_ssh_port=$(echo "$instance_json" | jq -r '.ssh_port // ""')
  if [ -z "$remote_ssh_port" ]; then
    remote_ssh_port="${REMOTE_SSH_PORT:-29418}"
  fi

  # Calculate ports (for local container)
  local http_port=$((BASE_HTTP_PORT + index))
  local ssh_port=$((BASE_SSH_PORT + index))

  # Get API path to mirror production server's URL structure
  # Compute this early so we can use it for both config and Docker env vars
  local api_path
  api_path=$(get_api_path "$slug")

  # Check if USE_API_PATH is enabled (default: false)
  # When false, we ignore api_path for local URLs but still store it for reference
  local effective_api_path=""
  if [ "${USE_API_PATH:-false}" = "true" ] && [ -n "$api_path" ]; then
    effective_api_path="$api_path"
  fi

  # Check for external tunnel configuration
  # When TUNNEL_HOST and TUNNEL_PORTS are set, use external URLs
  local tunnel_http_port=""
  local tunnel_ssh_port=""
  local use_tunnel="false"

  if [ -n "${TUNNEL_HOST:-}" ] && [ -n "${TUNNEL_PORTS:-}" ]; then
    # Validate TUNNEL_PORTS is valid JSON before parsing
    if ! echo "$TUNNEL_PORTS" | jq -e . >/dev/null 2>&1; then
      echo "  Warning: TUNNEL_PORTS is not valid JSON, falling back to localhost URLs"
    else
      # Extract tunnel ports for this slug from JSON
      tunnel_http_port=$(echo "$TUNNEL_PORTS" | jq -r --arg s "$slug" '.[$s].http // empty')
      tunnel_ssh_port=$(echo "$TUNNEL_PORTS" | jq -r --arg s "$slug" '.[$s].ssh // empty')

      # Validate extracted ports are numeric and in valid range (1-65535)
      validate_port() {
        local port="$1"
        if [[ "$port" =~ ^[0-9]+$ ]] && [ "$port" -ge 1 ] && [ "$port" -le 65535 ]; then
          return 0
        fi
        return 1
      }

      if [ -n "$tunnel_http_port" ] && [ -n "$tunnel_ssh_port" ]; then
        if ! validate_port "$tunnel_http_port"; then
          echo "  Warning: Invalid HTTP tunnel port '$tunnel_http_port', falling back to localhost URLs"
          tunnel_http_port=""
          tunnel_ssh_port=""
        elif ! validate_port "$tunnel_ssh_port"; then
          echo "  Warning: Invalid SSH tunnel port '$tunnel_ssh_port', falling back to localhost URLs"
          tunnel_http_port=""
          tunnel_ssh_port=""
        fi
      fi
    fi

    if [ -n "$tunnel_http_port" ] && [ -n "$tunnel_ssh_port" ]; then
      use_tunnel="true"
      echo "  External tunnel configured: ${TUNNEL_HOST}"
      echo "    HTTP port: $tunnel_http_port"
      echo "    SSH port: $tunnel_ssh_port"
    else
      echo "  Warning: TUNNEL_HOST set but no ports found for slug '$slug'"
      echo "  Falling back to localhost URLs"
    fi
  fi

  # Build URLs using effective_api_path (may be empty if USE_API_PATH is false)
  #
  # IMPORTANT: URL PATH CONFIGURATION - KEEP IN SYNC!
  # ================================================
  # The following files must be updated together if changing URL path handling:
  #
  #   1. scripts/start-instances.sh (this file)
  #      - canonical_url and listen_url variables below
  #      - CANONICAL_WEB_URL and HTTPD_LISTEN_URL env vars in docker run
  #
  #   2. scripts/check-services.sh
  #      - HEALTH_URL construction (must match listen_url path)
  #      - Plugin check URL (must match listen_url path)
  #
  #   3. test-gerrit-servers/.github/workflows/ (tunnel workflow)
  #      - Tunnel inputs configure public URLs
  #
  # When USE_API_PATH is true and api_path is detected, the local container
  # will use the same URL structure as the production server. This ensures:
  # - Clone URLs displayed in the web UI match the expected format
  # - Static content is served from the correct path prefix
  # - Health checks and API endpoints use consistent paths
  #
  # When USE_API_PATH is false (default), the container serves at root (/)
  # which avoids potential issues with static asset loading in PolyGerrit.
  #
  local canonical_url
  local listen_url
  local advertised_ssh_addr

  # Determine the host and ports for canonical URLs
  local url_host
  local url_http_port
  local url_ssh_port

  if [ "$use_tunnel" = "true" ]; then
    url_host="$TUNNEL_HOST"
    url_http_port="$tunnel_http_port"
    url_ssh_port="$tunnel_ssh_port"
  else
    url_host="localhost"
    url_http_port="$http_port"
    url_ssh_port="$ssh_port"
  fi

  # Build the advertised SSH address (used in clone URLs)
  advertised_ssh_addr="${url_host}:${url_ssh_port}"

  if [ -n "$effective_api_path" ]; then
    # Use api_path to match production server structure
    canonical_url="http://${url_host}:${url_http_port}${effective_api_path}/"
    listen_url="http://*:8080${effective_api_path}/"
    echo "  Using API path: $effective_api_path (USE_API_PATH=true)"
  else
    # No api_path - serve at root
    canonical_url="http://${url_host}:${url_http_port}/"
    listen_url="http://*:8080/"
    if [ -n "$api_path" ]; then
      echo "  API path detected ($api_path) but USE_API_PATH is false"
      echo "  Serving at root instead"
    fi
  fi

  # Export for use by other steps (e.g., tunnel config updates)
  {
    echo "GERRIT_CANONICAL_URL=$canonical_url"
    echo "GERRIT_LISTEN_URL=$listen_url"
    echo "GERRIT_SSH_ADDR=$advertised_ssh_addr"
    if [ "$use_tunnel" = "true" ]; then
      echo "GERRIT_TUNNEL_MODE=true"
    fi
  } >> "$WORK_DIR/env.sh"

  echo ""
  echo "========================================"
  echo "Instance $((index + 1)): $slug"
  echo "  Project: $project"
  echo "  Source: $gerrit_host"
  echo "  Local HTTP Port: $http_port"
  echo "  Local SSH Port: $ssh_port"
  if [ "$use_tunnel" = "true" ]; then
    echo "  Tunnel Mode: ENABLED"
    echo "  Public URL: $canonical_url"
    echo "  Public SSH: $advertised_ssh_addr"
  else
    echo "  Tunnel Mode: disabled (localhost)"
  fi
  echo "========================================"

  # Instance directory
  local instance_dir="$WORK_DIR/instances/$slug"

  # Initialize site (pass canonical_url for consistency)
  init_gerrit_site "$instance_dir" "$slug" "$http_port" "$canonical_url" || return 1

  # Configure Gerrit (pass pre-computed URLs and advertised SSH address)
  configure_gerrit "$instance_dir" "$slug" "$http_port" \
    "$canonical_url" "$listen_url" "$api_path" "$advertised_ssh_addr" || return 1

  # Download and install plugins
  download_plugin "$instance_dir/plugins" || return 1
  download_additional_plugins "$instance_dir/plugins" || return 1

  # Setup authentication
  if [ "$AUTH_TYPE" = "ssh" ]; then
    setup_ssh_auth "$instance_dir" "$gerrit_host" || return 1
  fi

  # Generate replication configuration
  # Note: pull-replication plugin uses replication.config (same as bundled plugin)
  # We remove the bundled replication.jar to avoid conflicts
  generate_replication_config "$instance_dir/etc/replication.config" \
    "$slug" "$gerrit_host" "$project" "$remote_ssh_user" "$remote_ssh_port"
  generate_secure_config "$instance_dir/etc/secure.config" "$slug"

  # Fetch project list from remote and pre-create directories for replication
  # Pre-creation is REQUIRED because fetchEvery mode only polls repos in projectCache
  fetch_expected_project_count "$instance_dir" "$project" "$gerrit_host" "$slug"

  # Remove the bundled replication plugin to avoid conflicts
  # The pull-replication plugin will be used instead
  rm -f "$instance_dir/plugins/replication.jar" 2>/dev/null || true

  # Start container
  echo "Starting Gerrit container..."

  local container_name="gerrit-$slug"
  local cidfile="$WORK_DIR/${container_name}.cid"

  local docker_args=(
    -d
    --name "$container_name"
    --cidfile "$cidfile"
    # NOTE: --rm removed so containers can be stopped/started without recreation
    # This allows config changes to persist across restarts
    -p "$http_port:8080"
    -p "$ssh_port:29418"
    -v "$instance_dir/git:/var/gerrit/git"
    -v "$instance_dir/cache:/var/gerrit/cache"
    -v "$instance_dir/index:/var/gerrit/index"
    -v "$instance_dir/data:/var/gerrit/data"
    -v "$instance_dir/etc:/var/gerrit/etc"
    -v "$instance_dir/logs:/var/gerrit/logs"
    -v "$instance_dir/plugins:/var/gerrit/plugins"
    -v "$instance_dir/tmp:/var/gerrit/tmp"
    # Pass CANONICAL_WEB_URL so the entrypoint sets it correctly
    # The Gerrit Docker entrypoint overwrites gerrit.config from this env var
    # on every container start. We must pass it to prevent the entrypoint from
    # defaulting to the container hostname.
    -e "CANONICAL_WEB_URL=$canonical_url"
    -e "HTTPD_LISTEN_URL=$listen_url"
  )

  # Add SSH volume if using SSH auth
  if [ "$AUTH_TYPE" = "ssh" ]; then
    docker_args+=(-v "$instance_dir/ssh:/var/gerrit/ssh:ro")
  fi

  # Add debug flag if enabled
  if [ "$DEBUG" = "true" ]; then
    docker_args+=(-e "DEBUG=1")
  fi

  docker run "${docker_args[@]}" \
    "${CUSTOM_IMAGE}" || {
    echo "::error::Failed to start Gerrit container for $slug ‚ùå"
    return 1
  }

  # Wait for container to start
  sleep 2

  # Get container ID and IP
  local cid
  cid=$(cat "$cidfile")
  local container_ip
  container_ip=$(docker inspect -f \
    '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' "$cid")

  echo "$cid" >> "$CONTAINER_IDS_FILE"

  # Get API path and URL for this instance
  local api_path
  local api_url
  api_path=$(get_api_path "$slug")
  api_url=$(get_api_url "$slug")

  # Capture SSH host keys from the container
  # These are auto-generated during Gerrit init
  local ssh_host_keys_dir="$WORK_DIR/ssh_host_keys/$slug"
  mkdir -p "$ssh_host_keys_dir"

  echo "Capturing SSH host public keys..."
  # Copy only public key files from the container to reduce secret exposure risk
  # Standard OpenSSH public key names: ssh_host_rsa_key.pub, ssh_host_ecdsa_key.pub, ssh_host_ed25519_key.pub
  # Private keys are not needed - we only use public keys for known_hosts
  local pub_key_files
  pub_key_files=$(docker exec "$cid" sh -c 'ls /var/gerrit/etc/ssh_host_*_key.pub 2>/dev/null' || echo "")
  for pub_key_path in $pub_key_files; do
    local pub_key_file
    pub_key_file=$(basename "$pub_key_path")
    docker cp "$cid:/var/gerrit/etc/${pub_key_file}" \
      "$ssh_host_keys_dir/${pub_key_file}" 2>/dev/null || true
  done

  # Build SSH host public keys JSON for this instance
  local ssh_host_pub_keys=""
  for pub_key_file in "$ssh_host_keys_dir"/*.pub; do
    if [ -f "$pub_key_file" ]; then
      local key_name
      key_name=$(basename "$pub_key_file" .pub)
      local key_content
      key_content=$(tr -d '\n' < "$pub_key_file")
      if [ -n "$ssh_host_pub_keys" ]; then
        ssh_host_pub_keys="${ssh_host_pub_keys},"
      fi
      ssh_host_pub_keys="${ssh_host_pub_keys}\"${key_name}\":\"${key_content}\""
    fi
  done
  ssh_host_pub_keys="{${ssh_host_pub_keys}}"
  echo "  SSH host keys captured ‚úÖ"

  # Read expected project count from earlier step
  local expected_project_count=0
  if [ -f "$instance_dir/expected_project_count" ]; then
    expected_project_count=$(cat "$instance_dir/expected_project_count" 2>/dev/null || echo "0")
  fi

  # Store instance metadata
  local temp_json
  temp_json=$(mktemp)
  jq --arg slug "$slug" \
     --arg cid "$cid" \
     --arg ip "$container_ip" \
     --argjson http_port "$http_port" \
     --argjson ssh_port "$ssh_port" \
     --arg url "http://$container_ip:8080" \
     --arg gerrit_host "$gerrit_host" \
     --arg project "$project" \
     --arg api_path "$api_path" \
     --arg api_url "$api_url" \
     --argjson expected_project_count "$expected_project_count" \
     --argjson ssh_host_keys "$ssh_host_pub_keys" \
     '.[$slug] = {
       cid: $cid,
       ip: $ip,
       http_port: $http_port,
       ssh_port: $ssh_port,
       url: $url,
       gerrit_host: $gerrit_host,
       project: $project,
       api_path: $api_path,
       api_url: $api_url,
       expected_project_count: $expected_project_count,
       ssh_host_keys: $ssh_host_keys
     }' \
     "$INSTANCES_JSON_FILE" > "$temp_json"
  mv "$temp_json" "$INSTANCES_JSON_FILE"

  echo "‚úÖ Gerrit instance started"
  echo "   Container ID: $cid"
  echo "   IP Address: $container_ip"
  echo "   HTTP URL: http://$container_ip:8080"
  echo "   SSH URL: ssh://localhost:$ssh_port"
  echo "   Source API URL: $api_url"
  echo ""

  # Log container status
  if [ "$DEBUG" = "true" ]; then
    echo "Container status:"
    docker ps -f "id=$cid"
    echo ""
  fi
}

# Main loop - start all instances
while IFS= read -r instance; do
  start_instance "$instance" "$INSTANCE_INDEX" || {
    echo "::error::Failed to start instance $INSTANCE_INDEX ‚ùå"
    exit 1
  }
  INSTANCE_INDEX=$((INSTANCE_INDEX + 1))
done <<< "$INSTANCES"

# Summary
echo "========================================"
echo "All instances started! ‚úÖ"
echo "Total instances: $INSTANCE_INDEX"
echo "========================================"
echo ""

# Add to step summary
{
  echo "**Instances Started** üöÄ"
  echo ""
  echo "| Slug | HTTP Port | SSH Port | Status |"
  echo "|------|-----------|----------|--------|"
} >> "$GITHUB_STEP_SUMMARY"

for slug in $(jq -r 'keys[]' "$INSTANCES_JSON_FILE"); do
  http_port=$(jq -r ".\"$slug\".http_port" "$INSTANCES_JSON_FILE")
  ssh_port=$(jq -r ".\"$slug\".ssh_port" "$INSTANCES_JSON_FILE")
  echo "| $slug | $http_port | $ssh_port | ‚úÖ Running |" \
    >> "$GITHUB_STEP_SUMMARY"
done

echo "" >> "$GITHUB_STEP_SUMMARY"
